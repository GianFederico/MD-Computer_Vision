{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOGYBhFUrS39gw7gd8ZVtKl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GianFederico/MD-repo-Computer_Vision/blob/main/NLP_lab1_2_wordnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTZ-NE9gmVh2",
        "outputId": "d4ec0c91-a8ef-456f-c836-9331e7d3d587"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n",
            "[nltk_data] Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/webtext.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Introductory Examples for the NLTK Book ***\n",
            "Loading text1, ..., text9 and sent1, ..., sent9\n",
            "Type the name of the text or sentence to view it.\n",
            "Type: 'texts()' or 'sents()' to list the materials.\n",
            "text1: Moby Dick by Herman Melville 1851\n",
            "text2: Sense and Sensibility by Jane Austen 1811\n",
            "text3: The Book of Genesis\n",
            "text4: Inaugural Address Corpus\n",
            "text5: Chat Corpus\n",
            "text6: Monty Python and the Holy Grail\n",
            "text7: Wall Street Journal\n",
            "text8: Personals Corpus\n",
            "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('popular')\n",
        "nltk.download('nps_chat')\n",
        "nltk.download('webtext')\n",
        "\n",
        "from nltk.book import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WordNet is a semantically-oriented dictionary of English, similar to a traditional thesaurus but with a richer structure\n",
        "# NLTK includes the English WordNet\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "for syn in wn.synsets('dish'):\n",
        "  print(syn)\n",
        "  print(syn.lemma_names())\n",
        "  print (syn.definition())\n",
        "  print(syn.examples())\n",
        "  print(\"___________________________________________________\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tms6tlxbm_IX",
        "outputId": "4df686dc-3605-4339-ef2d-19d479851c30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('dish.n.01')\n",
            "['dish']\n",
            "a piece of dishware normally used as a container for holding or serving food\n",
            "['we gave them a set of dishes for a wedding present']\n",
            "___________________________________________________\n",
            "Synset('dish.n.02')\n",
            "['dish']\n",
            "a particular item of prepared food\n",
            "['she prepared a special dish for dinner']\n",
            "___________________________________________________\n",
            "Synset('dish.n.03')\n",
            "['dish', 'dishful']\n",
            "the quantity that a dish will hold\n",
            "['they served me a dish of rice']\n",
            "___________________________________________________\n",
            "Synset('smasher.n.02')\n",
            "['smasher', 'stunner', 'knockout', 'beauty', 'ravisher', 'sweetheart', 'peach', 'lulu', 'looker', 'mantrap', 'dish']\n",
            "a very attractive or seductive looking woman\n",
            "[]\n",
            "___________________________________________________\n",
            "Synset('dish.n.05')\n",
            "['dish', 'dish_aerial', 'dish_antenna', 'saucer']\n",
            "directional antenna consisting of a parabolic reflector for microwave or radio frequency radiation\n",
            "[]\n",
            "___________________________________________________\n",
            "Synset('cup_of_tea.n.01')\n",
            "['cup_of_tea', 'bag', 'dish']\n",
            "an activity that you like or at which you are superior\n",
            "['chemistry is not my cup of tea', 'his bag now is learning to play golf', 'marriage was scarcely his dish']\n",
            "___________________________________________________\n",
            "Synset('serve.v.06')\n",
            "['serve', 'serve_up', 'dish_out', 'dish_up', 'dish']\n",
            "provide (usually but not necessarily food)\n",
            "['We serve meals for the homeless', 'She dished out the soup at 8 P.M.', 'The entertainers served up a lively show']\n",
            "___________________________________________________\n",
            "Synset('dish.v.02')\n",
            "['dish']\n",
            "make concave; shape like a dish\n",
            "[]\n",
            "___________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "motorcars=wn.synsets('motorcar')\n",
        "print (\"Exploring WordNet Hierarchy for: motorcar\") # it is a hyponym of 'motor veichle' and a hypernym of every results in the output\n",
        "for synset in motorcars:\n",
        "  print (\"motorcar sense:\", synset)\n",
        "  types_of_motorcars = synset.hyponyms()\n",
        "  i=1\n",
        "  for mc_specialization in types_of_motorcars:\n",
        "    print (\"motorcar specialization:\", i)\n",
        "    print (mc_specialization)\n",
        "    i+=1\n",
        "    print(\"______________________________\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvmh3TucpYGr",
        "outputId": "2db95c3b-8b55-41b2-e237-7398513f1528"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exploring WordNet Hierarchy for: motorcar\n",
            "motorcar sense: Synset('car.n.01')\n",
            "motorcar specialization: 1\n",
            "Synset('ambulance.n.01')\n",
            "______________________________\n",
            "motorcar specialization: 2\n",
            "Synset('beach_wagon.n.01')\n",
            "______________________________\n",
            "motorcar specialization: 3\n",
            "Synset('bus.n.04')\n",
            "______________________________\n",
            "motorcar specialization: 4\n",
            "Synset('cab.n.03')\n",
            "______________________________\n",
            "motorcar specialization: 5\n",
            "Synset('compact.n.03')\n",
            "______________________________\n",
            "motorcar specialization: 6\n",
            "Synset('convertible.n.01')\n",
            "______________________________\n",
            "motorcar specialization: 7\n",
            "Synset('coupe.n.01')\n",
            "______________________________\n",
            "motorcar specialization: 8\n",
            "Synset('cruiser.n.01')\n",
            "______________________________\n",
            "motorcar specialization: 9\n",
            "Synset('electric.n.01')\n",
            "______________________________\n",
            "motorcar specialization: 10\n",
            "Synset('gas_guzzler.n.01')\n",
            "______________________________\n",
            "motorcar specialization: 11\n",
            "Synset('hardtop.n.01')\n",
            "______________________________\n",
            "motorcar specialization: 12\n",
            "Synset('hatchback.n.01')\n",
            "______________________________\n",
            "motorcar specialization: 13\n",
            "Synset('horseless_carriage.n.01')\n",
            "______________________________\n",
            "motorcar specialization: 14\n",
            "Synset('hot_rod.n.01')\n",
            "______________________________\n",
            "motorcar specialization: 15\n",
            "Synset('jeep.n.01')\n",
            "______________________________\n",
            "motorcar specialization: 16\n",
            "Synset('limousine.n.01')\n",
            "______________________________\n",
            "motorcar specialization: 17\n",
            "Synset('loaner.n.02')\n",
            "______________________________\n",
            "motorcar specialization: 18\n",
            "Synset('minicar.n.01')\n",
            "______________________________\n",
            "motorcar specialization: 19\n",
            "Synset('minivan.n.01')\n",
            "______________________________\n",
            "motorcar specialization: 20\n",
            "Synset('model_t.n.01')\n",
            "______________________________\n",
            "motorcar specialization: 21\n",
            "Synset('pace_car.n.01')\n",
            "______________________________\n",
            "motorcar specialization: 22\n",
            "Synset('racer.n.02')\n",
            "______________________________\n",
            "motorcar specialization: 23\n",
            "Synset('roadster.n.01')\n",
            "______________________________\n",
            "motorcar specialization: 24\n",
            "Synset('sedan.n.01')\n",
            "______________________________\n",
            "motorcar specialization: 25\n",
            "Synset('sport_utility.n.01')\n",
            "______________________________\n",
            "motorcar specialization: 26\n",
            "Synset('sports_car.n.01')\n",
            "______________________________\n",
            "motorcar specialization: 27\n",
            "Synset('stanley_steamer.n.01')\n",
            "______________________________\n",
            "motorcar specialization: 28\n",
            "Synset('stock_car.n.01')\n",
            "______________________________\n",
            "motorcar specialization: 29\n",
            "Synset('subcompact.n.01')\n",
            "______________________________\n",
            "motorcar specialization: 30\n",
            "Synset('touring_car.n.01')\n",
            "______________________________\n",
            "motorcar specialization: 31\n",
            "Synset('used-car.n.01')\n",
            "______________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cars=wn.synsets('car')\n",
        "print (\"Exploring WordNet Hierarchy for: car\")\n",
        "for synset in cars:\n",
        "  print (\"car sense:\", synset)\n",
        "  types_of_cars = synset.hyponyms()\n",
        "  i=1\n",
        "  for c_specialization in types_of_cars:\n",
        "    print (\"car specialization:\", i)\n",
        "    print (c_specialization)\n",
        "    i+=1\n",
        "    print(\"______________________________\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CRSsm6qrKeR",
        "outputId": "2ccd0add-477b-43fe-b8ee-c0b2e67e71a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exploring WordNet Hierarchy for: car\n",
            "car sense: Synset('car.n.01')\n",
            "car specialization: 1\n",
            "Synset('ambulance.n.01')\n",
            "______________________________\n",
            "car specialization: 2\n",
            "Synset('beach_wagon.n.01')\n",
            "______________________________\n",
            "car specialization: 3\n",
            "Synset('bus.n.04')\n",
            "______________________________\n",
            "car specialization: 4\n",
            "Synset('cab.n.03')\n",
            "______________________________\n",
            "car specialization: 5\n",
            "Synset('compact.n.03')\n",
            "______________________________\n",
            "car specialization: 6\n",
            "Synset('convertible.n.01')\n",
            "______________________________\n",
            "car specialization: 7\n",
            "Synset('coupe.n.01')\n",
            "______________________________\n",
            "car specialization: 8\n",
            "Synset('cruiser.n.01')\n",
            "______________________________\n",
            "car specialization: 9\n",
            "Synset('electric.n.01')\n",
            "______________________________\n",
            "car specialization: 10\n",
            "Synset('gas_guzzler.n.01')\n",
            "______________________________\n",
            "car specialization: 11\n",
            "Synset('hardtop.n.01')\n",
            "______________________________\n",
            "car specialization: 12\n",
            "Synset('hatchback.n.01')\n",
            "______________________________\n",
            "car specialization: 13\n",
            "Synset('horseless_carriage.n.01')\n",
            "______________________________\n",
            "car specialization: 14\n",
            "Synset('hot_rod.n.01')\n",
            "______________________________\n",
            "car specialization: 15\n",
            "Synset('jeep.n.01')\n",
            "______________________________\n",
            "car specialization: 16\n",
            "Synset('limousine.n.01')\n",
            "______________________________\n",
            "car specialization: 17\n",
            "Synset('loaner.n.02')\n",
            "______________________________\n",
            "car specialization: 18\n",
            "Synset('minicar.n.01')\n",
            "______________________________\n",
            "car specialization: 19\n",
            "Synset('minivan.n.01')\n",
            "______________________________\n",
            "car specialization: 20\n",
            "Synset('model_t.n.01')\n",
            "______________________________\n",
            "car specialization: 21\n",
            "Synset('pace_car.n.01')\n",
            "______________________________\n",
            "car specialization: 22\n",
            "Synset('racer.n.02')\n",
            "______________________________\n",
            "car specialization: 23\n",
            "Synset('roadster.n.01')\n",
            "______________________________\n",
            "car specialization: 24\n",
            "Synset('sedan.n.01')\n",
            "______________________________\n",
            "car specialization: 25\n",
            "Synset('sport_utility.n.01')\n",
            "______________________________\n",
            "car specialization: 26\n",
            "Synset('sports_car.n.01')\n",
            "______________________________\n",
            "car specialization: 27\n",
            "Synset('stanley_steamer.n.01')\n",
            "______________________________\n",
            "car specialization: 28\n",
            "Synset('stock_car.n.01')\n",
            "______________________________\n",
            "car specialization: 29\n",
            "Synset('subcompact.n.01')\n",
            "______________________________\n",
            "car specialization: 30\n",
            "Synset('touring_car.n.01')\n",
            "______________________________\n",
            "car specialization: 31\n",
            "Synset('used-car.n.01')\n",
            "______________________________\n",
            "car sense: Synset('car.n.02')\n",
            "car specialization: 1\n",
            "Synset('baggage_car.n.01')\n",
            "______________________________\n",
            "car specialization: 2\n",
            "Synset('cabin_car.n.01')\n",
            "______________________________\n",
            "car specialization: 3\n",
            "Synset('club_car.n.01')\n",
            "______________________________\n",
            "car specialization: 4\n",
            "Synset('freight_car.n.01')\n",
            "______________________________\n",
            "car specialization: 5\n",
            "Synset('guard's_van.n.01')\n",
            "______________________________\n",
            "car specialization: 6\n",
            "Synset('handcar.n.01')\n",
            "______________________________\n",
            "car specialization: 7\n",
            "Synset('mail_car.n.01')\n",
            "______________________________\n",
            "car specialization: 8\n",
            "Synset('passenger_car.n.01')\n",
            "______________________________\n",
            "car specialization: 9\n",
            "Synset('slip_coach.n.01')\n",
            "______________________________\n",
            "car specialization: 10\n",
            "Synset('tender.n.04')\n",
            "______________________________\n",
            "car specialization: 11\n",
            "Synset('van.n.03')\n",
            "______________________________\n",
            "car sense: Synset('car.n.03')\n",
            "car sense: Synset('car.n.04')\n",
            "car sense: Synset('cable_car.n.01')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can also navigate up the hierarchy by visiting hypernyms\n",
        "# Some words have multiple paths, because they can be classified in more than one way\n",
        "# ‒ more than one hypernym\n",
        "# ‒ There are two paths between car.n.01 and entity.n.01 because wheeled_vehicle.n.01 can be classified as both a conveyance and a container. \n",
        "\n",
        "motorcar = wn.synset('car.n.01')\n",
        "print (motorcar.lemma_names())\n",
        "print (motorcar.definition())\n",
        "print (motorcar.hypernyms())\n",
        "paths = motorcar.hypernym_paths()\n",
        "print (\"Paths:\", len(paths))\n",
        "print ([synset.name() for synset in paths[0]])\n",
        "print ([synset.name() for synset in paths[1]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R85DpuZzrlqu",
        "outputId": "8d406911-679c-49c3-de6c-107ba2957443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['car', 'auto', 'automobile', 'machine', 'motorcar']\n",
            "a motor vehicle with four wheels; usually propelled by an internal combustion engine\n",
            "[Synset('motor_vehicle.n.01')]\n",
            "Paths: 2\n",
            "['entity.n.01', 'physical_entity.n.01', 'object.n.01', 'whole.n.02', 'artifact.n.01', 'instrumentality.n.03', 'container.n.01', 'wheeled_vehicle.n.01', 'self-propelled_vehicle.n.01', 'motor_vehicle.n.01', 'car.n.01']\n",
            "['entity.n.01', 'physical_entity.n.01', 'object.n.01', 'whole.n.02', 'artifact.n.01', 'instrumentality.n.03', 'conveyance.n.03', 'vehicle.n.01', 'wheeled_vehicle.n.01', 'self-propelled_vehicle.n.01', 'motor_vehicle.n.01', 'car.n.01']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can traverse the WordNet network to find synsets with related meanings \n",
        "# Knowing which words are semantically related is useful for indexing a collection of texts, so that a search for a general term like vehicle will match documents containing specific terms like limousine\n",
        "right = wn.synset('right_whale.n.01')\n",
        "orca = wn.synset('orca.n.01')\n",
        "minke = wn.synset('minke_whale.n.01')\n",
        "tortoise = wn.synset('tortoise.n.01')\n",
        "novel = wn.synset('novel.n.01')\n",
        "print ('right_whale.n.01 - orca.n.01',\n",
        "right.lowest_common_hypernyms(orca))\n",
        "print ('right_whale.n.01 - minke_whale.n.01',\n",
        "right.lowest_common_hypernyms(minke))\n",
        "print ('right_whale.n.01 - tortoise.n.01',\n",
        "right.lowest_common_hypernyms(tortoise))\n",
        "print ('right_whale.n.01 - novel.n.01',\n",
        "right.lowest_common_hypernyms(novel))\n",
        "\n",
        "# Of course we know that whale is very specific (and baleen whale even more so), while vertebrate is more general and # entity is completely general.\n",
        "# We can quantify this concept of generality by looking up the depth of each synset\n",
        "wn.synset('baleen_whale.n.01').min_depth() #Returns the length of the shortest hypernym path from this synset to the root\n",
        "\n",
        "print (wn.synset('baleen_whale.n.01'),wn.synset('baleen_whale.n.01').min_depth()) # ->most specific synset similar to right_whale (higher min.depth)\n",
        "print (wn.synset('whale.n.02'), wn.synset('whale.n.02').min_depth())\n",
        "print (wn.synset('vertebrate.n.01'),wn.synset('vertebrate.n.01').min_depth())\n",
        "print (wn.synset('entity.n.01'), wn.synset('entity.n.01').min_depth())\n",
        "\n",
        "print(\"______________________________\")\n",
        "print ('right_whale.n.01 - orca.n.01', right.path_similarity(orca))\n",
        "print ('right_whale.n.01 - minke_whale.n.01', right.path_similarity(minke))\n",
        "print ('right_whale.n.01 - tortoise.n.01', right.path_similarity(tortoise))\n",
        "print ('right_whale.n.01 - novel.n.01', right.path_similarity(novel))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiar4D-zsPMX",
        "outputId": "42bc587e-960a-49f2-e088-1d91fc55967c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "right_whale.n.01 - orca.n.01 [Synset('whale.n.02')]\n",
            "right_whale.n.01 - minke_whale.n.01 [Synset('baleen_whale.n.01')]\n",
            "right_whale.n.01 - tortoise.n.01 [Synset('vertebrate.n.01')]\n",
            "right_whale.n.01 - novel.n.01 [Synset('entity.n.01')]\n",
            "Synset('baleen_whale.n.01') 14\n",
            "Synset('whale.n.02') 13\n",
            "Synset('vertebrate.n.01') 8\n",
            "Synset('entity.n.01') 0\n",
            "______________________________\n",
            "right_whale.n.01 - orca.n.01 0.16666666666666666\n",
            "right_whale.n.01 - minke_whale.n.01 0.25\n",
            "right_whale.n.01 - tortoise.n.01 0.07692307692307693\n",
            "right_whale.n.01 - novel.n.01 0.043478260869565216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a conditional frequency distribution over the Names corpus that allows you to see which initial letters are more frequent for males vs. females\n",
        "names = nltk.corpus.names\n",
        "\n",
        "print (\"initial most common letter\")\n",
        "from nltk.corpus import names #get the text\n",
        "cfd = nltk.ConditionalFreqDist(\n",
        "    (target, fileid[:1])  #the 2 axes\n",
        "    for fileid in names.fileids()\n",
        "    for w in names.words(fileid)  #all words in each file\n",
        "    for target in ['a', 'b', 'c','d', 'e', 'f','g', 'h', 'i','j', 'k', 'l','m', 'n', 'o','p', 'q', 'r','s', 't', 'u','v','w', 'y', 'x','z']\n",
        "    if w.lower().startswith(target))  #narrow the choice\n",
        "letters = ['a', 'b', 'c','d', 'e', 'f','g', 'h', 'i','j', 'k', 'l','m', 'n', 'o','p', 'q', 'r','s', 't', 'u','v','w', 'y', 'x','z']  \n",
        "genders = [fileid[:1] for fileid in names.fileids()]\n",
        "cfd.tabulate(conditions=letters, samples=genders[:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufIQxyRWt-z_",
        "outputId": "c002b1ec-af15-4735-abc1-c5b676590acf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial most common letter\n",
            "    f   m \n",
            "a 443 213 \n",
            "b 246 173 \n",
            "c 469 166 \n",
            "d 308 146 \n",
            "e 251 119 \n",
            "f 144  87 \n",
            "g 213 156 \n",
            "h 124 163 \n",
            "i  83  45 \n",
            "j 293 144 \n",
            "k 276  70 \n",
            "l 332 113 \n",
            "m 484 200 \n",
            "n 158  77 \n",
            "o  66  52 \n",
            "p 121 101 \n",
            "q   9  15 \n",
            "r 247 200 \n",
            "s 309 238 \n",
            "t 198 188 \n",
            "u  14  22 \n",
            "v 105  50 \n",
            "w  54 151 \n",
            "y  18  16 \n",
            "x   5   7 \n",
            "z  31  31 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function supergloss(s) that takes a synset s as its argument and returns a string consisting of the concatenation of the definition of s, and the definitions of all the hypernyms and hyponyms of s\n",
        "def supergloss(syn):\n",
        "  string=[]\n",
        "  for synset in syn:\n",
        "    print(\"sense->\", synset)\n",
        "    string.append(synset.definition())\n",
        "    hypons = synset.hyponyms()\n",
        "    hyperns = synset.hypernyms()\n",
        "    \n",
        "    for hypo in hypons:\n",
        "      string.append(hypo.definition())\n",
        "\n",
        "    for hype in hyperns:\n",
        "      string.append(hype.definition())\n",
        "  print(\"__________\")\n",
        "  return string\n",
        "\n",
        "food=wn.synsets('landscape')\n",
        "supergloss(food)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kV_qMQtcImwU",
        "outputId": "d1e726ab-449f-4333-93cd-e4e1cc538916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sense-> Synset('landscape.n.01')\n",
            "sense-> Synset('landscape.n.02')\n",
            "sense-> Synset('landscape.n.03')\n",
            "sense-> Synset('landscape.n.04')\n",
            "sense-> Synset('landscape.v.01')\n",
            "sense-> Synset('landscape.v.02')\n",
            "__________\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['an expanse of scenery that can be seen in a single view',\n",
              " 'the appearance of a place',\n",
              " 'painting depicting an expanse of natural scenery',\n",
              " 'graphic art consisting of an artistic composition made by applying paints to a surface',\n",
              " 'a genre of art dealing with the depiction of natural scenery',\n",
              " 'a class of art (or artistic endeavor) having a characteristic form or technique',\n",
              " 'an extensive mental viewpoint',\n",
              " 'a mental position from which things are viewed',\n",
              " 'embellish with plants',\n",
              " 'make more attractive by adding ornament, colour, etc.',\n",
              " 'do landscape gardening',\n",
              " 'work in the garden']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a function that finds the 50 most frequently occurring words of a text that are not stopwords\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('popular')\n",
        "nltk.download('nps_chat')\n",
        "nltk.download('webtext')\n",
        "\n",
        "from nltk.book import *\n",
        "\n",
        "def frequent_words(text):\n",
        "  stopwords= nltk.corpus.stopwords.words('english') #english stopwords list\n",
        "  content=[w for w in text if w.lower() not in stopwords]\n",
        "  fdist = nltk.FreqDist(w.lower() for w in content)\n",
        "  return fdist.most_common(50)\n",
        "\n",
        "print (\"Brown:\", frequent_words(nltk.corpus.brown.words()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDHITpyZOWzL",
        "outputId": "81f8518f-8133-4139-d775-de5c891cfa3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n",
            "[nltk_data] Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]   Package nps_chat is already up-to-date!\n",
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Package webtext is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Brown: [(',', 58334), ('.', 49346), ('``', 8837), (\"''\", 8789), (';', 5566), ('?', 4693), ('--', 3432), ('one', 3292), ('would', 2714), (')', 2466), ('(', 2435), ('said', 1961), (':', 1795), ('new', 1635), ('could', 1601), ('time', 1598), ('!', 1596), ('two', 1412), ('may', 1402), ('first', 1361), ('like', 1292), ('man', 1207), ('even', 1170), ('made', 1125), ('also', 1069), ('many', 1030), ('must', 1013), ('af', 996), ('back', 966), ('years', 950), ('much', 937), ('way', 908), ('well', 897), ('people', 847), ('mr.', 844), ('little', 831), ('state', 807), ('good', 806), ('make', 794), ('world', 787), ('still', 782), ('see', 772), ('men', 763), ('work', 762), ('long', 752), ('get', 749), ('life', 715), ('never', 697), ('day', 687), ('another', 684)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a function word_freq() that takes a word and the name of a section of the Brown Corpus as arguments, and computes the frequency of the word in that section of the corpus\n",
        "\n",
        "def word_freq(word, section):\n",
        "  #text=nltk.corpus.brown.words(section)\n",
        "  text=nltk.corpus.brown.words(categories=section)\n",
        "  fdist = nltk.FreqDist(w.lower() for w in text)\n",
        "  freq=fdist[word]\n",
        "  return freq\n",
        "\n",
        "print(nltk.corpus.brown.fileids())\n",
        "print(nltk.corpus.brown.categories())\n",
        "\n",
        "word_freq(\"and\", \"adventure\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvruN_mpRy5w",
        "outputId": "fad0e534-8f41-417c-ff79-2ec925b55a9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ca01', 'ca02', 'ca03', 'ca04', 'ca05', 'ca06', 'ca07', 'ca08', 'ca09', 'ca10', 'ca11', 'ca12', 'ca13', 'ca14', 'ca15', 'ca16', 'ca17', 'ca18', 'ca19', 'ca20', 'ca21', 'ca22', 'ca23', 'ca24', 'ca25', 'ca26', 'ca27', 'ca28', 'ca29', 'ca30', 'ca31', 'ca32', 'ca33', 'ca34', 'ca35', 'ca36', 'ca37', 'ca38', 'ca39', 'ca40', 'ca41', 'ca42', 'ca43', 'ca44', 'cb01', 'cb02', 'cb03', 'cb04', 'cb05', 'cb06', 'cb07', 'cb08', 'cb09', 'cb10', 'cb11', 'cb12', 'cb13', 'cb14', 'cb15', 'cb16', 'cb17', 'cb18', 'cb19', 'cb20', 'cb21', 'cb22', 'cb23', 'cb24', 'cb25', 'cb26', 'cb27', 'cc01', 'cc02', 'cc03', 'cc04', 'cc05', 'cc06', 'cc07', 'cc08', 'cc09', 'cc10', 'cc11', 'cc12', 'cc13', 'cc14', 'cc15', 'cc16', 'cc17', 'cd01', 'cd02', 'cd03', 'cd04', 'cd05', 'cd06', 'cd07', 'cd08', 'cd09', 'cd10', 'cd11', 'cd12', 'cd13', 'cd14', 'cd15', 'cd16', 'cd17', 'ce01', 'ce02', 'ce03', 'ce04', 'ce05', 'ce06', 'ce07', 'ce08', 'ce09', 'ce10', 'ce11', 'ce12', 'ce13', 'ce14', 'ce15', 'ce16', 'ce17', 'ce18', 'ce19', 'ce20', 'ce21', 'ce22', 'ce23', 'ce24', 'ce25', 'ce26', 'ce27', 'ce28', 'ce29', 'ce30', 'ce31', 'ce32', 'ce33', 'ce34', 'ce35', 'ce36', 'cf01', 'cf02', 'cf03', 'cf04', 'cf05', 'cf06', 'cf07', 'cf08', 'cf09', 'cf10', 'cf11', 'cf12', 'cf13', 'cf14', 'cf15', 'cf16', 'cf17', 'cf18', 'cf19', 'cf20', 'cf21', 'cf22', 'cf23', 'cf24', 'cf25', 'cf26', 'cf27', 'cf28', 'cf29', 'cf30', 'cf31', 'cf32', 'cf33', 'cf34', 'cf35', 'cf36', 'cf37', 'cf38', 'cf39', 'cf40', 'cf41', 'cf42', 'cf43', 'cf44', 'cf45', 'cf46', 'cf47', 'cf48', 'cg01', 'cg02', 'cg03', 'cg04', 'cg05', 'cg06', 'cg07', 'cg08', 'cg09', 'cg10', 'cg11', 'cg12', 'cg13', 'cg14', 'cg15', 'cg16', 'cg17', 'cg18', 'cg19', 'cg20', 'cg21', 'cg22', 'cg23', 'cg24', 'cg25', 'cg26', 'cg27', 'cg28', 'cg29', 'cg30', 'cg31', 'cg32', 'cg33', 'cg34', 'cg35', 'cg36', 'cg37', 'cg38', 'cg39', 'cg40', 'cg41', 'cg42', 'cg43', 'cg44', 'cg45', 'cg46', 'cg47', 'cg48', 'cg49', 'cg50', 'cg51', 'cg52', 'cg53', 'cg54', 'cg55', 'cg56', 'cg57', 'cg58', 'cg59', 'cg60', 'cg61', 'cg62', 'cg63', 'cg64', 'cg65', 'cg66', 'cg67', 'cg68', 'cg69', 'cg70', 'cg71', 'cg72', 'cg73', 'cg74', 'cg75', 'ch01', 'ch02', 'ch03', 'ch04', 'ch05', 'ch06', 'ch07', 'ch08', 'ch09', 'ch10', 'ch11', 'ch12', 'ch13', 'ch14', 'ch15', 'ch16', 'ch17', 'ch18', 'ch19', 'ch20', 'ch21', 'ch22', 'ch23', 'ch24', 'ch25', 'ch26', 'ch27', 'ch28', 'ch29', 'ch30', 'cj01', 'cj02', 'cj03', 'cj04', 'cj05', 'cj06', 'cj07', 'cj08', 'cj09', 'cj10', 'cj11', 'cj12', 'cj13', 'cj14', 'cj15', 'cj16', 'cj17', 'cj18', 'cj19', 'cj20', 'cj21', 'cj22', 'cj23', 'cj24', 'cj25', 'cj26', 'cj27', 'cj28', 'cj29', 'cj30', 'cj31', 'cj32', 'cj33', 'cj34', 'cj35', 'cj36', 'cj37', 'cj38', 'cj39', 'cj40', 'cj41', 'cj42', 'cj43', 'cj44', 'cj45', 'cj46', 'cj47', 'cj48', 'cj49', 'cj50', 'cj51', 'cj52', 'cj53', 'cj54', 'cj55', 'cj56', 'cj57', 'cj58', 'cj59', 'cj60', 'cj61', 'cj62', 'cj63', 'cj64', 'cj65', 'cj66', 'cj67', 'cj68', 'cj69', 'cj70', 'cj71', 'cj72', 'cj73', 'cj74', 'cj75', 'cj76', 'cj77', 'cj78', 'cj79', 'cj80', 'ck01', 'ck02', 'ck03', 'ck04', 'ck05', 'ck06', 'ck07', 'ck08', 'ck09', 'ck10', 'ck11', 'ck12', 'ck13', 'ck14', 'ck15', 'ck16', 'ck17', 'ck18', 'ck19', 'ck20', 'ck21', 'ck22', 'ck23', 'ck24', 'ck25', 'ck26', 'ck27', 'ck28', 'ck29', 'cl01', 'cl02', 'cl03', 'cl04', 'cl05', 'cl06', 'cl07', 'cl08', 'cl09', 'cl10', 'cl11', 'cl12', 'cl13', 'cl14', 'cl15', 'cl16', 'cl17', 'cl18', 'cl19', 'cl20', 'cl21', 'cl22', 'cl23', 'cl24', 'cm01', 'cm02', 'cm03', 'cm04', 'cm05', 'cm06', 'cn01', 'cn02', 'cn03', 'cn04', 'cn05', 'cn06', 'cn07', 'cn08', 'cn09', 'cn10', 'cn11', 'cn12', 'cn13', 'cn14', 'cn15', 'cn16', 'cn17', 'cn18', 'cn19', 'cn20', 'cn21', 'cn22', 'cn23', 'cn24', 'cn25', 'cn26', 'cn27', 'cn28', 'cn29', 'cp01', 'cp02', 'cp03', 'cp04', 'cp05', 'cp06', 'cp07', 'cp08', 'cp09', 'cp10', 'cp11', 'cp12', 'cp13', 'cp14', 'cp15', 'cp16', 'cp17', 'cp18', 'cp19', 'cp20', 'cp21', 'cp22', 'cp23', 'cp24', 'cp25', 'cp26', 'cp27', 'cp28', 'cp29', 'cr01', 'cr02', 'cr03', 'cr04', 'cr05', 'cr06', 'cr07', 'cr08', 'cr09']\n",
            "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1706"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}